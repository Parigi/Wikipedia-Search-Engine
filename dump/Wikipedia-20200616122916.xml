<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.35.0-wmf.36</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Estimation statistics</title>
    <ns>0</ns>
    <id>40703875</id>
    <revision>
      <id>955144874</id>
      <parentid>950703705</parentid>
      <timestamp>2020-05-06T05:27:49Z</timestamp>
      <contributor>
        <username>Upisaida</username>
        <id>4788901</id>
      </contributor>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="16597" xml:space="preserve">{{distinguish|Estimation theory}}
{{other uses | Estimation (disambiguation)}}

'''Estimation statistics''' is a data analysis framework that uses a combination of [[effect size]]s, [[confidence interval]]s, precision planning, and [[meta-analysis]] to plan experiments, analyze data and interpret results.&lt;ref&gt;{{cite web|last=Ellis|first=Paul|title=Effect size FAQ|url=http://effectsizefaq.com/}}&lt;/ref&gt; It is distinct from [[Statistical hypothesis testing|null hypothesis significance testing]] (NHST), which is considered to be less informative.&lt;ref name=":0"&gt;{{cite web|last=Cohen|first=Jacob|title=The earth is round (p&lt;.05)|url=https://www.ics.uci.edu/~sternh/courses/210/cohen94_pval.pdf}}&lt;/ref&gt;&lt;ref name="cumming" /&gt; Estimation statistics, or simply '''estimation''', is also known as '''the new statistics''',&lt;ref name="cumming" /&gt; a distinction introduced in the fields of [[psychology]], [[medical research]], life sciences and a wide range of other experimental sciences where NHST still remains prevalent,&lt;ref name="button" /&gt; despite estimation statistics having been recommended as preferable for several decades.&lt;ref&gt;{{cite book|last=Altman|first=Douglas|title=Practical Statistics For Medical Research|url=https://archive.org/details/isbn_9780412276309|url-access=registration|year=1991|publisher=Chapman and Hall|location=London}}&lt;/ref&gt;&lt;ref name=":1"&gt;{{cite book|title=Statistics with Confidence|year=2000|publisher=Wiley-Blackwell|location=London|editor=Douglas Altman}}&lt;/ref&gt;

The primary aim of estimation methods is to report an [[effect size]] (a [[point estimate]]) along with its [[confidence interval]], the latter of which is related to the precision of the estimate.&lt;ref name="cohen" /&gt; The confidence interval summarizes a range of likely values of the underlying population effect. Proponents of estimation see reporting a [[P-value|''P'' value]] as an unhelpful distraction from the important business of reporting an effect size with its confidence intervals,&lt;ref&gt;{{cite web|last=Ellis|first=Paul|title=Why can't I just judge my result by looking at the p value?|url=http://effectsizefaq.com/2010/05/31/why-can%E2%80%99t-i-just-judge-my-result-by-looking-at-the-p-value/|accessdate=5 June 2013|date=2010-05-31}}&lt;/ref&gt; and believe that estimation should replace significance testing for data analysis.&lt;ref&gt;{{Cite journal|last=Claridge-Chang|first=Adam|last2=Assam|first2=Pryseley N|title=Estimation statistics should replace significance testing|journal=Nature Methods|volume=13|issue=2|pages=108–109|doi=10.1038/nmeth.3729|pmid=26820542|year=2016|url=https://zenodo.org/record/60156/files/Estimationmethods-eprint.pdf}}&lt;/ref&gt;

==History==
Physics has for long employed a weighted averages method that is similar to [[meta-analysis]].&lt;ref&gt;{{cite journal|last=Hedges|first=Larry|title=How hard is hard science, how soft is soft science|journal=American Psychologist|year=1987|volume=42|issue=5|page=443|doi=10.1037/0003-066x.42.5.443|citeseerx=10.1.1.408.2317}}&lt;/ref&gt;

Estimation statistics in the modern era started with the development of the [[effect size|standardized effect size]] by [[Jacob Cohen (statistician)|Jacob Cohen]] in the 1960s. Research synthesis using estimation statistics was pioneered by [[Gene V. Glass]] with the development of the method of [[meta-analysis]] in the 1970s.&lt;ref&gt;{{cite book|last=Hunt|first=Morton|title=How science takes stock: the story of meta-analysis|year=1997|publisher=The Russell Sage Foundation|location=New York|isbn=978-0-87154-398-1}}&lt;/ref&gt; Estimation methods have been refined since by [[Larry Hedges]], Michael Borenstein, [[Doug Altman]], Martin Gardner, Geoff Cumming and others. The [[systematic review]], in conjunction with meta-analysis, is a related technique with widespread use in medical research. There are now over 60,000 citations to "meta-analysis" in [[PubMed]]. Despite the widespread adoption of meta-analysis, the estimation framework is still not routinely used in primary biomedical research.&lt;ref name="button"&gt;{{cite journal|last=Button|first=Katherine |author2=John P. A. Ioannidis |author3=Claire Mokrysz |author4=Brian A. Nosek |author5=Jonathan Flint |author6=Emma S. J. Robinson |author7=Marcus R. Munafò|title=Power failure: why small sample size undermines the reliability of neuroscience|journal=Nature Reviews Neuroscience|year=2013|volume=14|issue=5 |pages=365–76 |doi=10.1038/nrn3475 |pmid=23571845|doi-access=free }}&lt;/ref&gt;

In the 1990s, editor [http://www.bu.edu/sph/profile/kenneth-rothman/ Kenneth Rothman] banned the use of p-values from the journal [[Epidemiology (journal)|''Epidemiology'']]; compliance was high among authors but this did not substantially change their analytical thinking.&lt;ref&gt;{{cite journal|last=Fidler|first=Fiona|authorlink=Fiona Fidler|title=Editors Can Lead Researchers to Confidence Intervals, but Can't Make Them Think|journal=Psychological Science|volume=15|issue=2|pages=119–126|url=http://pss.sagepub.com/content/15/2/119.abstract|doi=10.1111/j.0963-7214.2004.01502008.x|pmid=14738519|year=2004}}&lt;/ref&gt;

More recently, estimation methods are being adopted in fields such as neuroscience,&lt;ref&gt;{{Cite journal|last=Yildizoglu|first=Tugce|last2=Weislogel|first2=Jan-Marek|last3=Mohammad|first3=Farhan|last4=Chan|first4=Edwin S.-Y.|last5=Assam|first5=Pryseley N.|last6=Claridge-Chang|first6=Adam|date=2015-12-08|title=Estimating Information Processing in a Memory System: The Utility of Meta-analytic Methods for Genetics|journal=PLOS Genet|volume=11|issue=12|pages=e1005718|doi=10.1371/journal.pgen.1005718|issn=1553-7404|pmc=4672901|pmid=26647168}}&lt;/ref&gt; psychology education&lt;ref&gt;{{cite journal|last=Hentschke|first=Harald|author2=Maik C. Stüttgen|title=Computation of measures of effect size for neuroscience data sets|journal=European Journal of Neuroscience|date=December 2011|volume=34|issue=12|pages=1887–1894|doi=10.1111/j.1460-9568.2011.07902.x|pmid=22082031}}&lt;/ref&gt; and psychology.&lt;ref&gt;{{cite web|last=Cumming|first=Geoff|title=ESCI (Exploratory Software for Confidence Intervals)|url=http://www.latrobe.edu.au/psy/research/projects/esci}}&lt;/ref&gt;

The Publication Manual of the American Psychological Association recommends estimation over hypothesis testing.&lt;ref&gt;{{cite web|title=Publication Manual of the American Psychological Association, Sixth Edition|url=http://www.apastyle.org/manual/index.aspx|accessdate=17 May 2013}}&lt;/ref&gt; The Uniform Requirements for Manuscripts Submitted to Biomedical Journals document makes a similar recommendation: "Avoid relying solely on statistical hypothesis testing, such as P values, which fail to convey important information about effect size."&lt;ref&gt;{{cite web|title=Uniform Requirements for Manuscripts Submitted to Biomedical Journals|url=http://www.icmje.org/manuscript_1prepare.html|accessdate=17 May 2013|url-status=dead|archiveurl=https://web.archive.org/web/20130515225111/http://www.icmje.org/manuscript_1prepare.html|archivedate=15 May 2013}}&lt;/ref&gt;

In 2019, the [[Society for Neuroscience]] journal [[eNeuro]] instituted a policy recommending the use of estimation graphics as the preferred method for data presentation &lt;ref&gt;{{cite web|title=Changing the Way We Report, Interpret, and Discuss Our Results to Rebuild Trust in Our Research|url=https://www.eneuro.org/content/6/4/ENEURO.0259-19.2019}}&lt;/ref&gt;

==Methodology==
Many significance tests have an estimation counterpart;&lt;ref&gt;{{Cite book|title=Introduction to the New Statistics: Estimation, Open Science, and Beyond|last=Cumming|first=Geoff|last2=Calin-Jageman|first2=Robert|publisher=Routledge|year=2016|isbn=978-1138825529|location=|pages=}}&lt;/ref&gt; in almost every case, the test result (or its [[p-value]]) can be simply substituted with the effect size and a precision estimate. For example, instead of using [[Student's t-test]], the analyst can compare two independent groups by calculating the mean difference and its 95% [[confidence interval]]. Corresponding methods can be used for a [[paired t-test]] and multiple comparisons. Similarly, for a regression analysis, an analyst would report the [[coefficient of determination]] (R&lt;sup&gt;2&lt;/sup&gt;) and the model equation instead of the model's p-value.

However, proponents of estimation statistics warn against reporting only a few numbers. Rather, it is advised to analyze and present data using data visualization.&lt;ref name=":0" /&gt;&lt;ref name=":1" /&gt;&lt;ref name="cohen" /&gt; Examples of appropriate visualizations include the [[Scatter plot]] for regression, and Gardner-Altman plots for two independent groups.&lt;ref name=":2"&gt;{{Cite journal|last=Gardner|first=M. J.|last2=Altman|first2=D. G.|date=1986-03-15|title=Confidence intervals rather than P values: estimation rather than hypothesis testing|journal=British Medical Journal (Clinical Research Ed.)|volume=292|issue=6522|pages=746–750|issn=0267-0623|pmc=1339793|pmid=3082422}}&lt;/ref&gt; While historical data-group plots (bar charts, box plots, and violin plots) do not display the comparison, estimation plots add a second axis to explicitly visualize the effect size.&lt;ref&gt;{{Cite journal|url=https://www.biorxiv.org/content/early/2018/07/26/377978|title=Moving beyond P values: Everyday data analysis with estimation plots|journal=bioRxiv|pages=377978|last=Ho|first=Joses|last2=Tumkaya|last3=Aryal|last4=Choi|last5=Claridge-Chang|doi=10.1101/377978|year=2018|doi-access=free}}&lt;/ref&gt;

[[File:20171231-wiki-figure-png.png|thumb|'''The Gardner–Altman plot. Left:''' A conventional bar chart, using asterisks to show that the difference is 'statistically significant.' '''Right:''' A Gardner–Altman plot that shows all data points, along with the mean difference and its confidence intervals.]]

===Gardner–Altman plot===
The Gardner–Altman mean difference plot was first described by [[Martin Gardner]] and [[Doug Altman]] in 1986;&lt;ref name=":2" /&gt; it is a statistical graph designed to display data from two independent groups.&lt;ref name=":1" /&gt; There is also a version suitable for [http://www.estimationstats.com/#/analyze/paired paired data]. The key instructions to make this chart are as follows: (1) display all observed values for both groups side-by-side; (2) place a second axis on the right, shifted to show the mean difference scale; and (3) plot the mean difference with its confidence interval as a marker with error bars.&lt;ref name="cumming" /&gt; Gardner-Altman plots can be generated  with custom code using [[Ggplot2]], [https://seaborn.pydata.org/generated/seaborn.swarmplot.html seaborn], or [https://github.com/ACCLAB/DABEST-python DABEST]; alternatively, the analyst can use user-friendly software like the [http://www.estimationstats.com/#/ Estimation Stats] app.

[[File:Cumming_Estimation_Plot.png|thumb|right|'''The Cumming plot.''' All raw data is shown. The effect size and 95% CIs are plotted on a separate axes beneath the raw data. For each group, summary measurements (mean ± standard deviation) are shown as gapped lines.]]

=== Cumming plot ===
For multiple groups, [http://www.latrobe.edu.au/psychology/staff/profile?uname=GDCumming Geoff Cumming] introduced the use of a secondary panel to plot two or more mean differences and their confidence intervals, placed below the observed values panel;&lt;ref name="cumming" /&gt; this arrangement enables [http://www.estimationstats.com/#/analyze/multi easy comparison] of mean differences ('deltas') over several data groupings. Cumming plots can be generated with the [https://thenewstatistics.com/itns/esci/ ESCI package], [https://github.com/ACCLAB/DABEST-python DABEST], or the [http://www.estimationstats.com/#/ Estimation Stats app].

=== Other methodologies ===
In addition to the mean difference, there are numerous other [[effect size]] types, all with relative benefits. Major types include Cohen's d-type effect sizes, and the [[coefficient of determination]] (R&lt;sup&gt;2&lt;/sup&gt;) for [[regression analysis]]. For non-normal distributions, there are a number of more [https://garstats.wordpress.com/2016/05/02/robust-effect-sizes-for-2-independent-groups/ robust effect sizes], including [[Effect size#Effect size for ordinal data|Cliff's delta]] and the [[Kolmogorov-Smirnov statistic]].

==Flaws in hypothesis testing==
{{main|Statistical hypothesis testing#Criticism}}
{{see also|p-value#Criticism|Misuse of p-values}}

In [[Statistical hypothesis testing|hypothesis testing]], the primary objective of statistical calculations is to obtain a [[p-value]], the probability of seeing an obtained result, or a more extreme result, when assuming the [[null hypothesis]] is true. If the p-value is low (usually &lt; 0.05), the statistical practitioner is then encouraged to reject the null hypothesis. Proponents of '''estimation''' reject the validity of hypothesis testing&lt;ref name="cumming"&gt;{{cite book|last=Cumming|first=Geoff|title=Understanding The New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analysis|year=2012|publisher=Routledge|location=New York}}&lt;/ref&gt;&lt;ref name="cohen"&gt;{{cite journal|last=Cohen|first=Jacob|title=What I have Learned (So Far)|journal=American Psychologist|year=1990|volume=45|issue=12|page=1304|doi=10.1037/0003-066x.45.12.1304}}&lt;/ref&gt; for the following reasons, among others:

* P-values are easily and commonly misinterpreted. For example, the p-value is often mistakenly thought of as 'the probability that the null hypothesis is true.'
* The null hypothesis is always wrong for every set of observations: there is always some effect, even if it is minuscule.&lt;ref name=earth&gt;{{cite journal|last=Cohen|first=Jacob|title=The earth is round (p &lt; .05).|journal=American Psychologist|year=1994|volume=49|issue=12|pages=997–1003|doi=10.1037/0003-066X.49.12.997}}&lt;/ref&gt;
* Hypothesis testing produces arbitrarily dichotomous yes-no answers, while discarding important information about magnitude.&lt;ref&gt;{{cite book|last=Ellis|first=Paul|title=The Essential Guide to Effect Sizes: Statistical Power, Meta-Analysis, and the Interpretation of Research Results|year=2010|publisher=Cambridge University Press|location=Cambridge}}&lt;/ref&gt;
* Any particular p-value arises through the interaction of the [[effect size]], the [[sample size]] (all things being equal a larger sample size produces a smaller p-value) and sampling error.&lt;ref&gt;{{cite book|title=The Significance Test Controversy: A Reader|year=2006|publisher=Aldine Transaction|isbn=978-0202308791|editor=Denton E. Morrison, Ramon E. Henkel}}&lt;/ref&gt;
*At low power, simulation reveals that sampling error makes p-values extremely volatile.&lt;ref&gt;{{cite web|last=Cumming|first=Geoff|title=Dance of the p values|url=https://www.youtube.com/watch?v=ez4DgdurRPg}}&lt;/ref&gt;

==Benefits of estimation statistics==

===Advantages of confidence intervals===
Confidence intervals behave in a predictable way. By definition, 95% confidence intervals have a 95% chance of capturing the underlying population mean (μ). This feature remains constant with increasing sample size; what changes is that the interval becomes smaller (more precise). In addition, 95% confidence intervals are also 83% prediction intervals: one experiment's confidence interval has an 83% chance of capturing any future experiment's mean.&lt;ref name="cumming" /&gt; As such, knowing a single experiment's 95% confidence intervals gives the analyst a plausible range for the population mean, and plausible outcomes of any subsequent replication experiments.

===Evidence-based statistics===
Psychological studies of the perception of statistics reveal that reporting interval estimates leaves a more accurate perception of the data than reporting p-values.&lt;ref&gt;{{cite journal|last=Beyth-Marom|first=R|author2=Fidler, F. |author3=Cumming, G. |title=Statistical cognition: Towards evidence-based practice in statistics and statistics education|journal=Statistics Education Research Journal|year=2008|volume=7|pages=20–39|accessdate=}}&lt;/ref&gt;

===Precision planning===
The precision of an estimate is formally defined as 1/[[variance]], and like power, increases (improves) with increasing sample size. Like [[statistical power|power]], a high level of precision is expensive; research grant applications would ideally include precision/cost analyses. Proponents of estimation believe precision planning should replace [[statistical power|power]] since statistical power itself is conceptually linked to significance testing.&lt;ref name="cumming" /&gt;

==See also==
{{Portal|Mathematics}}
* [[Effect size]]
* [[Cohen's h]]
* [[Interval estimation]]
* [[Meta-analysis]]
* [[Statistical significance]]

==References==
{{Reflist|30em}}

{{Statistics}}

[[Category:Estimation theory]]
[[Category:Effect size]]</text>
      <sha1>mqlsg13vwetzsatcsz38xc2qa5tk2c5</sha1>
    </revision>
  </page>
</mediawiki>
