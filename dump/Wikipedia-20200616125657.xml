<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.35.0-wmf.36</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Confusion matrix</title>
    <ns>0</ns>
    <id>847558</id>
    <revision>
      <id>961611475</id>
      <parentid>961469501</parentid>
      <timestamp>2020-06-09T13:12:50Z</timestamp>
      <contributor>
        <ip>141.20.65.106</ip>
      </contributor>
      <comment>re-add the table on the right, the template was temporarily broken</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="5611" xml:space="preserve">{{short description|Table layout for visualizing performance; also called an error matrix}}
{{Confusion matrix terms|recall=}}
In the field of [[machine learning]] and specifically the problem of [[statistical classification]], a '''confusion matrix''', also known as an error matrix,&lt;ref&gt;{{cite journal |last1=Stehman |first1= Stephen V. |year= 1997|title=Selecting and interpreting measures of thematic classification accuracy |journal=Remote Sensing of Environment |volume=62 |issue=1 |pages=77–89 |doi= 10.1016/S0034-4257(97)00083-7 |bibcode= 1997RSEnv..62...77S }}&lt;/ref&gt; is a specific table layout that allows visualization of the performance of an algorithm, typically a [[supervised learning]] one (in [[unsupervised learning]] it is usually called a '''matching matrix'''). Each row of the [[matrix (mathematics)|matrix]] represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).&lt;ref name="Powers2011"&gt;{{cite journal |first=David M W |last=Powers |date=2011 |title=Evaluation: From Precision, Recall and F-Measure  to ROC, Informedness, Markedness &amp; Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37–63 |url=http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf}}&lt;/ref&gt; The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).

It is a special kind of [[contingency table]], with two dimensions ("actual" and "predicted"), and identical sets of "classes" in both dimensions (each combination of dimension and class is a variable in the contingency table).

==Example==
If a classification system has been trained to distinguish between cats and dogs, a confusion matrix will summarize the results of testing the algorithm for further inspection. Assuming a sample of 13 animals &amp;mdash; 8 cats and 5 dogs &amp;mdash; the resulting confusion matrix could look like the table below:
{|
|-
|
{| class="wikitable" style="border:none; float:left; margin-top:0;"
!style="background:white; border:none;" colspan="2" rowspan="2"|
!colspan="2" style="background:none;"| Actual class
|-
!Cat
!Dog
|-
!rowspan="2" style="height:6em;"|&lt;div style="{{transform-rotate|-90}}"&gt;Predicted&lt;br&gt;class&lt;/div&gt;
!Cat
|'''5'''
|2
|-
!Dog
|3
|'''3'''
|-
|}
|
|}
In this confusion matrix, of the 8 actual cats, the system predicted that three were dogs, and of the five dogs, it predicted that two were cats. All correct predictions are located in the diagonal of the table (highlighted in bold), so it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal.

In abstract terms, the confusion matrix is as follows:
{|
|-
|
{| class="wikitable" style="border:none; float:left; margin-top:0;"
!style="background:white; border:none;" colspan="2" rowspan="2"|
!colspan="2" style="background:none;"| Actual class
|-
!P
!N
|-
!rowspan="2" style="height:6em;"|&lt;div style="{{transform-rotate|-90}}"&gt;Predicted&lt;br&gt;class&lt;/div&gt;
!P
|'''TP'''
|FP
|-
!N
|FN
|'''TN'''
|-
|}
|
|}
where: P = positive; N = Negative; TP = True Positive; FP = False Positive; TN = True Negative; FN = False Negative.

==Table of confusion==
In [[predictive analytics]], a '''table of confusion''' (sometimes also called a '''confusion matrix'''), is a table with two rows and two columns that reports the number of ''false positives'', ''false negatives'', ''true positives'', and ''true negatives''. This allows more detailed analysis than mere proportion of correct classifications (accuracy). Accuracy will yield misleading results if the data set is unbalanced; that is, when the numbers of observations in different classes vary greatly. For example, if there were 95 cats and only 5 dogs in the data, a particular classifier might classify all the observations as cats. The overall accuracy would be 95%, but in more detail the classifier would have a 100% recognition rate ([[sensitivity (test)|sensitivity]]) for the cat class but a 0% recognition rate for the dog class. [[F1 score]] is even more unreliable in such cases, and here would yield over 97.4%, whereas [[informedness]] removes such bias and yields 0 as the probability of an informed decision for any form of guessing (here always guessing cat).

According to Davide Chicco and Giuseppe Jurman, the most informative metric to evaluate a confusion matrix is the [[Matthews correlation coefficient|Matthews correlation coefficient (MCC)]]&lt;ref name="ChiccoJurman2020" /&gt;.

Assuming the confusion matrix above, its corresponding table of confusion, for the cat class, would be:

{| class="wikitable" style="border:none; margin-top:0;"
!style="background:white; border:none;" colspan="2" rowspan="2"|
!colspan="3" style="background:none;"| Actual class
|-
!Cat
!Non-cat
|-
!rowspan="3" style="height:6em;"|&lt;div style="{{rotate|-90}}"&gt;Predicted&lt;br&gt; class&lt;/div&gt;
!Cat
|5 True Positives
|2 False Positives
|-
!Non-cat
|3 False Negatives
|3 True Negatives
|-
|}

The final table of confusion would contain the average values for all classes combined.

Let us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 2×2 ''confusion matrix'', as follows:
{{DiagnosticTesting Diagram}}

==References==
{{Reflist}}


{{Matrix classes}}

[[Category:Machine learning]]
[[Category:Statistical classification]]

[[de:Beurteilung eines Klassifikators#Wahrheitsmatrix: Richtige und falsche Klassifikationen]]</text>
      <sha1>ar8xmg4av5n3sbfvmfi7p2f1odpftc0</sha1>
    </revision>
  </page>
</mediawiki>
