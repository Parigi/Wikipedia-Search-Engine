<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.35.0-wmf.36</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Least absolute deviations</title>
    <ns>0</ns>
    <id>19048902</id>
    <revision>
      <id>961645682</id>
      <parentid>960495507</parentid>
      <timestamp>2020-06-09T17:04:37Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Bluelink 1 book for [[Wikipedia:Verifiability|verifiability]] (prndis)) #IABot (v2.0.1) ([[User:GreenC bot|GreenC bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="15583" xml:space="preserve">{{Regression bar}}
'''Least absolute deviations''' ('''LAD'''), also known as '''least absolute errors''' ('''LAE'''), '''least absolute value''' ('''LAV'''), '''least absolute residual''' ('''LAR'''), '''sum of absolute deviations''', or the [[L1 norm|''L''&lt;sub&gt;1&lt;/sub&gt; norm]] condition, is a statistical [[optimality criterion]] and the statistical [[optimization (mathematics)|optimization]] technique that relies on it. Similar to the [[least squares]] technique, it attempts to find a [[function (mathematics)|function]] which closely approximates a set of data. In the simple case of a set of (''x'',''y'') data, the approximation function is a simple "trend line" in two-dimensional [[Cartesian coordinates]]. The method [[maxima and minima|minimizes]] the sum of absolute errors (SAE) (the sum of the absolute values of the vertical "residuals" between points generated by the function and corresponding points in the data). The least absolute deviations estimate also arises as the [[maximum likelihood]] estimate if the errors have a [[Laplace distribution]]. It was introduced in 1757 by [[Roger Joseph Boscovich]].&lt;ref&gt;{{cite book|chapter=Least Absolute Deviation Regression|title=The Concise Encyclopedia of Statistics|url=https://archive.org/details/conciseencyclope00dodg|url-access=limited|pages=[https://archive.org/details/conciseencyclope00dodg/page/n299 299]–302|doi=10.1007/978-0-387-32833-1_225|publisher=Springer|date=2008 |isbn=9780387328331}}&lt;/ref&gt;

==Formulation==

Suppose that the [[data set]] consists of the points (''x''&lt;sub&gt;''i''&lt;/sub&gt;, ''y''&lt;sub&gt;''i''&lt;/sub&gt;) with ''i'' = 1, 2, ..., ''n''. We want to find a function ''f'' such that &lt;math&gt;f(x_i)\approx y_i.&lt;/math&gt;

To attain this goal, we suppose that the function ''f'' is of a particular form containing some parameters which need to be determined. For instance, the simplest form would be linear: ''f''(''x'') = ''bx'' + ''c'', where ''b'' and ''c'' are parameters whose values are not known but which we would like to estimate. Less simply, suppose that ''f''(''x'') is [[quadratic function|quadratic]], meaning that ''f''(''x'') = ''ax''&lt;sup&gt;2&lt;/sup&gt; + ''bx'' + ''c'', where ''a'', ''b'' and ''c'' are not yet known. (More generally, there could be not just one explanator ''x'', but rather multiple explanators, all appearing as arguments of the function ''f''.)

We now seek estimated values of the unknown parameters that minimize the sum of the absolute values of the residuals:

:&lt;math&gt; S = \sum_{i=1}^n |y_i - f(x_i)|. &lt;/math&gt;

==Solution==

Though the idea of least absolute deviations regression is just as straightforward as that of least squares regression, the least absolute deviations line is not as simple to compute efficiently. Unlike least squares regression, least absolute deviations regression does not have an analytical solving method. Therefore, an iterative approach is required. The following is an enumeration of some least absolute deviations solving methods.

* [[Simplex algorithm|Simplex-based methods]] (such as the Barrodale-Roberts algorithm&lt;ref&gt;{{Cite journal
 | author = I. Barrodale &amp; F. D. K. Roberts
 | title = An improved algorithm for discrete L&lt;sub&gt;1&lt;/sub&gt; linear approximation
 | journal = [[SIAM Journal on Numerical Analysis]]
 | volume = 10
 | year = 1973
 | pages = 839–848
 | jstor = 2156318
 | doi = 10.1137/0710069
 | issue = 5
|bibcode = 1973SJNA...10..839B | hdl = 1828/11491
 | hdl-access = free
 }}&lt;/ref&gt;)
** Because the problem is a [[linear program]], any of the many linear programming techniques (including the simplex method as well as others) can be applied.
* [[Iteratively re-weighted least squares]]&lt;ref&gt;{{Cite journal
 | author = E. J. Schlossmacher
 | title = An Iterative Technique for Absolute Deviations Curve Fitting
 | journal = [[Journal of the American Statistical Association]]
 | volume = 68
 | issue = 344
 |date=December 1973
 | pages = 857–859
 | jstor = 2284512
 | doi = 10.2307/2284512
 }}&lt;/ref&gt;
* Wesolowsky’s direct descent method&lt;ref&gt;{{Cite journal
 | author = G. O. Wesolowsky
 | year = 1981
 | title = A new descent algorithm for the least absolute value regression problem
 | journal = Communications in Statistics – Simulation and Computation
 | volume = B10
 | issue = 5
 | pages = 479–491
 | doi = 10.1080/03610918108812224
}}&lt;/ref&gt;
* Li-Arce’s maximum likelihood approach&lt;ref&gt;{{Cite journal
 |author  = Yinbo Li and Gonzalo R. Arce
 |title   = A Maximum Likelihood Approach to Least Absolute Deviation Regression
 |journal = [[EURASIP Journal on Applied Signal Processing]]
 |volume  = 2004
 |year    = 2004
 |issue   = 12
 |pages   = 1762–1769
 |doi     = 10.1155/S1110865704401139
 |url     = http://www.hindawi.com/journals/asp/2004/948982.abs.html
|bibcode= 2004EJASP2004...61L
 |doi-access= free
 }}{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;
* Recursive reduction of dimensionality approach&lt;ref&gt;{{Cite journal
 |author  = Ana Sovic Krzic and Damir Sersic
 |title   = L1 minimization using recursive reduction of dimensionality
 |journal = Signal Processing
 |volume  = 151
 |year    = 2018
 |pages   = 119–129
 |doi     = 10.1016/j.sigpro.2018.05.002
 }}&lt;/ref&gt;
* Check all combinations of point-to-point lines for minimum sum of errors

Simplex-based methods are the “preferred” way to solve the least absolute deviations problem.&lt;ref name=Pfeil&gt;William A. Pfeil,
''[http://www.wpi.edu/Pubs/E-project/Available/E-project-050506-091720/unrestricted/IQP_Final_Report.pdf Statistical Teaching Aids]'', Bachelor of Science thesis, [[Worcester Polytechnic Institute]], 2006&lt;/ref&gt; A Simplex method is a method for solving a problem in linear programming. The most popular algorithm is the Barrodale-Roberts modified Simplex algorithm. The algorithms for IRLS, Wesolowsky's Method, and Li's Method can be found in Appendix A of &lt;ref name=Pfeil/&gt;
among other methods. Checking all combinations of lines traversing any two (x,y) data points is another method of finding the least absolute deviations line. Since it is known that at least one least absolute deviations line traverses at least two data points, this method will find a line by comparing the SAE (Smallest Absolute Error over data points) of each line, and choosing the line with the smallest SAE. In addition, if multiple lines have the same, smallest SAE, then the lines outline the region of multiple solutions. Though simple, this final method is inefficient for large sets of data.

===Using linear programming===

The problem can be solved using any linear programming technique on the following problem specification. We wish to

:&lt;math&gt; \text{Minimize} \sum_{i=1}^n |y_i - a_0 - a_1x_{i1} - a_2x_{i2} - \cdots - a_kx_{ik}|&lt;/math&gt;

with respect to the choice of the values of the parameters &lt;math&gt;a_0,\ldots, a_k&lt;/math&gt;, where ''y''&lt;sub&gt;''i''&lt;/sub&gt; is the value of the ''i''&lt;sup&gt;th&lt;/sup&gt; observation of the dependent variable, and ''x''&lt;sub&gt;''ij''&lt;/sub&gt; is the value of the ''i''&lt;sup&gt;th&lt;/sup&gt; observation of the ''j''&lt;sup&gt;th&lt;/sup&gt; independent variable (''j'' = 1,...,''k''). We rewrite this problem in terms of artificial variables ''u''&lt;sub&gt;''i''&lt;/sub&gt; as

:&lt;math&gt; \text{Minimize} \sum_{i=1}^n u_i&lt;/math&gt;

:with respect to &lt;math&gt;a_0,\ldots, a_k&lt;/math&gt; and &lt;math&gt;u_1,\ldots, u_n&lt;/math&gt;

:subject to

:&lt;math&gt; u_i \ge y_i - a_0 - a_1x_{i1} - a_2x_{i2} - \cdots - a_kx_{ik} \,\ \,\ \,\ \,\ \,\ \text{for } i=1,\ldots,n&lt;/math&gt;

:&lt;math&gt; u_i \ge -[y_i - a_0 - a_1x_{i1} - a_2x_{i2} - \cdots - a_kx_{ik}] \,\ \,\ \text{ for } i=1,\ldots,n.&lt;/math&gt;

These constraints have the effect of forcing each &lt;math&gt;u_i&lt;/math&gt; to equal &lt;math&gt;|y_i - a_0 - a_1x_{i1} - a_2x_{i2} - \cdots - a_kx_{ik}|&lt;/math&gt; upon being minimized, so the objective function is equivalent to the original objective function. Since this version of the problem statement does not contain the absolute value operator, it is in a format that can be solved with any linear programming package.

==Properties==

There exist other unique properties of the least absolute deviations line. In the case of a set of (''x'',''y'') data, the least absolute deviations line will always pass through at least two of the data points, unless there are multiple solutions. If multiple solutions exist, then the region of valid least absolute deviations solutions will be bounded by at least two lines, each of which passes through at least two data points. More generally, if there are ''k'' [[Dependent and independent variables#Alternative terminology in statistics|regressors]] (including the constant), then at least one optimal regression surface will pass through ''k'' of the data points.&lt;ref&gt;Branham, R. L., Jr., "Alternatives to least squares", ''[[Astronomical Journal]]'' 87, June 1982, 928–937. [http://adsabs.harvard.edu/full/1982AJ.....87..928B] at SAO/NASA Astrophysics Data System (ADS)&lt;/ref&gt;{{rp|p.936}}

This "latching" of the line to the data points can help to understand the "instability" property: if the line always latches to at least two points, then the line will jump between different sets of points as the data points are altered. The "latching" also helps to understand the "robustness" property: if there exists an outlier, and a least absolute deviations line must latch onto two data points, the outlier will most likely not be one of those two points because that will not minimize the sum of absolute deviations in most cases.

One known case in which multiple solutions exist is a set of points symmetric about a horizontal line, as shown in Figure A below.

[[File:Least absolute deviations regression method diagram.gif|600px|thumb|center|Figure A: A set of data points with reflection symmetry and multiple least absolute deviations solutions. The “solution area” is shown in green. The vertical blue lines represent the absolute errors from the pink line to each data point. The pink line is one of infinitely many solutions within the green area.]]

To understand why there are multiple solutions in the case shown in Figure A, consider the pink line in the green region. Its sum of absolute errors is some value S. If one were to tilt the line upward slightly, while still keeping it within the green region, the sum of errors would still be S. It would not change because the distance from each point to the line grows on one side of the line, while the distance to each point on the opposite side of the line diminishes by exactly the same amount. Thus the sum of absolute errors remains the same. Also, since one can tilt the line in infinitely small increments, this also shows that if there is more than one solution, there are infinitely many solutions.

===Advantages and disadvantages===

The following is a table contrasting some properties of the method of least absolute deviations with those of the method of least squares (for non-singular problems).&lt;ref&gt;For a set of applets that demonstrate these differences, see the following site: http://www.math.wpi.edu/Course_Materials/SAS/lablets/7.3/73_choices.html&lt;/ref&gt;&lt;ref&gt;For a discussion of LAD versus OLS, see these academic papers and reports: http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf and https://www.leeds.ac.uk/educol/documents/00003759.htm&lt;/ref&gt;

{| border="1" cellpadding="5" cellspacing="0"
|-
! Ordinary least squares regression || Least absolute deviations regression
|-
| rowspan=1 align="center"| Not very robust
| colspan=2 align="center"| Robust
|-
| rowspan=1 align="center"| Stable solution
| colspan=2 align="center"| Unstable solution
|-
| rowspan=1 align="center"| Always one solution
| colspan=2 align="center"| Possibly multiple solutions
|-
|}

The method of least absolute deviations finds applications in many areas, due to its robustness compared to the least squares method. Least absolute deviations is robust in that it is resistant to outliers in the data.  LAD gives equal emphasis to all observations, in contrast to ordinary least squares (OLS) which, by squaring the residuals, gives more weight to large residuals, that is, outliers in which predicted values are far from actual observations.  This may be helpful in studies where outliers do not need to be given greater weight than other observations. If it is important to give greater weight to outliers, the method of least squares is a better choice.

==Variations, extensions, specializations==
The least absolute deviation problem may be extended to include multiple explanators, constraints and [[regularization (mathematics)|regularization]], e.g., a linear model with linear constraints:&lt;ref&gt;{{Cite journal |author1=Mingren Shi |authorlink1=Mingren Shi |last2=Mark A. |first2= Lukas |authorlink2=Mark A. Lukas  | date=March 2002 | title = An ''L&lt;sub&gt;1&lt;/sub&gt;'' estimation algorithm with degeneracy and linear constraints
 | journal = [[Computational Statistics &amp; Data Analysis]]
 | doi = 10.1016/S0167-9473(01)00049-4
 | volume = 39
 | issue = 1
 | pages = 35–55
|url=http://researchrepository.murdoch.edu.au/id/eprint/15195/ }}&lt;/ref&gt;
: minimize &lt;math&gt;S(\mathbf{\beta}, b) = \sum_i | \mathbf{x}'_i \mathbf{\beta} + b - y_i |&lt;/math&gt;
: subject to, e.g., &lt;math&gt;\mathbf{x}'_1 \mathbf{\beta} + b - y_1 \leq  k&lt;/math&gt;

where &lt;math&gt;\mathbf{\beta}&lt;/math&gt; is a column vector of coefficients to be estimated, ''b'' is an intercept to be estimated, '''x'''&lt;sub&gt;'''i''' &lt;/sub&gt; is a column vector of the ''i''&lt;sup&gt;th&lt;/sup&gt; observations on the various explanators, ''y''&lt;sub&gt;''i''&lt;/sub&gt; is the ''i''&lt;sup&gt;th&lt;/sup&gt; observation on the dependent variable, and ''k'' is a known constant.

[[Regularization (mathematics)|Regularization]] with [[Lasso (statistics)|LASSO]] may also be combined with LAD.&lt;ref&gt;{{Cite conference
 | author = Li Wang, Michael D. Gordon &amp; Ji Zhu
 | title = Regularized Least Absolute Deviations Regression and an Efficient Algorithm for Parameter Tuning
 | booktitle = Proceedings of the Sixth International Conference on Data Mining
 |date=December 2006
 | pages = 690–700
 | doi = 10.1109/ICDM.2006.134
}}&lt;/ref&gt;

==See also==
* [[Quantile regression]]
* [[Regression analysis]]
* [[Linear regression model]]
* [[Absolute deviation]]
* [[Average absolute deviation]]
* [[Median absolute deviation]]
* [[Ordinary least squares]]
* [[Robust regression]]

==References==
{{Reflist|30em}}

==Further reading==
* {{Cite journal
 | author = Peter Bloomfield and William Steiger
 | title = Least Absolute Deviations Curve-Fitting
 | journal = [[SIAM Journal on Scientific Computing]]
 | year = 1980
 | volume = 1
 | issue = 2
 | pages =290–301
 | doi = 10.1137/0901019
}}
* {{Cite journal
 | author = Subhash C. Narula and John F. Wellington
 | title = The Minimum Sum of Absolute Errors Regression: A State of the Art Survey
 | journal = [[International Statistical Review]]
 | volume = 50
 | issue = 3
 | year = 1982
 | pages = 317–326
 | jstor = 1402501
 | doi = 10.2307/1402501
 }}
* {{Cite journal
 | author = Robert F. Phillips
 | title = Least absolute deviations estimation via the EM algorithm
 | journal =  [[Statistics and Computing]]
 | volume = 12
 | issue = 3
 |date=July 2002
 | doi = 10.1023/A:1020759012226
 | pages = 281–285
}}
* {{Cite journal
 | author = Enno Siemsen &amp; Kenneth A. Bollen
 | title = Least Absolute Deviation Estimation in Structural Equation Modeling
 | journal = [[Sociological Methods &amp; Research]]
 | volume = 36
 | issue = 2
 | pages = 227–265
 | year = 2007
 | doi = 10.1177/0049124107301946
 }}

{{DEFAULTSORT:Least Absolute Deviations}}
[[Category:Least squares]]
[[Category:Robust statistics]]
[[Category:Robust regression]]
[[Category:Point estimation performance]]</text>
      <sha1>chniki3cubxg9wljm2l2yglg2u5owyz</sha1>
    </revision>
  </page>
</mediawiki>
