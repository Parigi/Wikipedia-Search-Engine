<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.35.0-wmf.36</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Learning to rank</title>
    <ns>0</ns>
    <id>25050663</id>
    <revision>
      <id>959850980</id>
      <parentid>958947484</parentid>
      <timestamp>2020-05-30T22:46:10Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor/>
      <comment>/* List of methods */Replaced [[arXiv]] PDF link with more mobile-friendly abstract link, replaced: https://arxiv.org/pdf/ → https://arxiv.org/abs/</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="33592" xml:space="preserve">{{machine learning bar}}
'''Learning to rank'''&lt;ref name="liu"&gt;{{citation
|author=Tie-Yan Liu
|title=Learning to Rank for Information Retrieval
|journal=Foundations and Trends in Information Retrieval
|year=2009
|isbn=978-1-60198-244-5
|doi=10.1561/1500000016
|pages=225–331
|volume=3
|issue=3
}}. Slides from Tie-Yan Liu's talk at [[World Wide Web Conference|WWW]] 2009 conference are [http://wwwconference.org/www2009/pdf/T7A-LEARNING%20TO%20RANK%20TUTORIAL.pdf available online]
&lt;/ref&gt; or '''machine-learned ranking''' ('''MLR''') is the application of [[machine learning]], typically [[Supervised learning|supervised]], [[Semi-supervised learning|semi-supervised]] or [[reinforcement learning]], in the construction of [[ranking function|ranking models]] for [[information retrieval]] systems.&lt;ref&gt;[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The
MIT Press {{ISBN|9780262018258}}.&lt;/ref&gt; [[Training data]] consists of lists of items with some [[partial order]] specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model  purposes to rank, i.e. producing a [[permutation]] of items in new, unseen lists in a similar way to rankings in the training data.

== Applications ==

=== In information retrieval ===
[[File:MLR-search-engine-example.png|250px|thumb|A possible architecture of a machine-learned search engine.]]
Ranking is a central part of many [[information retrieval]] problems, such as [[document retrieval]], [[collaborative filtering]], [[sentiment analysis]], and [[online advertising]].

A possible architecture of a machine-learned search engine is shown in the accompanying figure.

Training data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human ''assessors'' (or ''raters'', as [[Google]] calls them),
&lt;!-- "assessor" is the more standard term, used e.g. by TREC conference --&gt;
who check results for some queries and determine [[Relevance (information retrieval)|relevance]] of each result. It is not feasible to check the relevance of all documents, and so typically a technique called pooling is used — only the top few documents, retrieved by some existing ranking models are checked. &lt;!--
  TODO: write something about selection bias caused by pooling
--&gt; Alternatively, training data may be derived automatically by analyzing ''clickthrough logs'' (i.e. search results which got clicks from users),&lt;ref name="Joachims2002"&gt;{{citation
 | author=Joachims, T.
 | journal=Proceedings of the ACM Conference on Knowledge Discovery and Data Mining
 | url=http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf
 | title=Optimizing Search Engines using Clickthrough Data
 | year=2002
}}&lt;/ref&gt; ''query chains'',&lt;ref&gt;{{citation
 |author1=Joachims T. |author2=Radlinski F. | title=Query Chains: Learning to Rank from Implicit Feedback
 | url=http://radlinski.org/papers/Radlinski05QueryChains.pdf
 | year=2005
 | journal=Proceedings of the ACM Conference on Knowledge Discovery and Data Mining
|arxiv=cs/0605035 |bibcode=2006cs........5035R }}&lt;/ref&gt; or such search engines' features as Google's [[Google SearchWiki|SearchWiki]].

Training data is used by a learning algorithm to produce a ranking model which computes the relevance of documents for actual queries.

Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.&lt;ref&gt;{{citation
 |author1=B. Cambazoglu |author2=H. Zaragoza |author3=O. Chapelle |author4=J. Chen |author5=C. Liao |author6=Z. Zheng |author7=J. Degenhardt. | title=Early exit optimizations for additive machine learned ranking systems
 | journal=WSDM '10: Proceedings of the Third ACM International Conference on Web Search and Data Mining, 2010. 
 | url=http://olivier.chapelle.cc/pub/wsdm2010.pdf
}}&lt;/ref&gt; First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the [[vector space model]], [[Standard Boolean model|boolean model]], weighted AND,&lt;ref&gt;{{citation
 | author1=Broder A.
 | author2=Carmel D.
 | author3=Herscovici M.
 | author4=Soffer A.
 | author5=Zien J.
 | title=Efficient query evaluation using a two-level retrieval process
 | journal=Proceedings of the Twelfth International Conference on Information and Knowledge Management
 | year=2003
 | pages=426–434
 | isbn=978-1-58113-723-1
 | url=http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf
 | access-date=2009-12-15
 | archive-url=https://web.archive.org/web/20090521102038/http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf
 | archive-date=2009-05-21
 | url-status=dead
 }}&lt;/ref&gt; or [[Okapi BM25|BM25]]. This phase is called ''top-&lt;math&gt;k&lt;/math&gt; document retrieval'' and many heuristics were proposed in the literature to accelerate it, such as using a document's static quality score and tiered indexes.&lt;ref name="manning-q-eval"&gt;{{citation
 |author1=Manning C. |author2=Raghavan P. |author3=Schütze H. | title=Introduction to Information Retrieval
 | publisher=Cambridge University Press
 | year=2008}}. Section [http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html 7.1]&lt;/ref&gt; In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.

=== In other areas ===
Learning to rank algorithms have been applied in areas other than information retrieval:
* In [[machine translation]] for ranking a set of hypothesized translations;&lt;ref name="Duh09"&gt;{{citation
 | author=Kevin K. Duh
 | title=Learning to Rank with {{sic|hide=y|Partially|-}}Labeled Data
 | year=2009
 | url=http://ssli.ee.washington.edu/people/duh/thesis/uwthesis.pdf
}}&lt;/ref&gt;
* In [[computational biology]] for ranking candidate 3-D structures in protein structure prediction problem.&lt;ref name="Duh09" /&gt;
* In [[recommender system]]s for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.&lt;ref&gt;Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and Yi Chang, [http://sifaka.cs.uiuc.edu/~ylv2/pub/www11-relatedness.pdf ''Learning to Model Relatedness for News Recommendation''] {{Webarchive|url=https://web.archive.org/web/20110827065356/http://sifaka.cs.uiuc.edu/~ylv2/pub/www11-relatedness.pdf |date=2011-08-27 }}, in International Conference on World Wide Web (WWW), 2011.&lt;/ref&gt;
* In [[software engineering]], learning-to-rank methods have been used for fault localization.&lt;ref&gt;{{Cite book|doi = 10.1109/ICSME.2014.41|chapter = Learning to Combine Multiple Ranking Metrics for Fault Localization|title = 2014 IEEE International Conference on Software Maintenance and Evolution|pages = 191–200|year = 2014|last1 = Xuan|first1 = Jifeng|last2 = Monperrus|first2 = Martin|citeseerx = 10.1.1.496.6829|chapter-url=https://hal.archives-ouvertes.fr/hal-01018935/document|isbn = 978-1-4799-6146-7}}&lt;/ref&gt;

== Feature vectors ==
For the convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called ''[[feature vector]]s''. Such an approach is sometimes called ''bag of features'' and is analogous to the [[bag of words]] model and [[vector space model]] used in information retrieval for representation of documents.

Components of such vectors are called ''[[feature (machine learning)|feature]]s'', ''factors'' or ''ranking signals''. They may be divided into three groups (features from [[document retrieval]] are shown as examples):
* ''Query-independent'' or ''static'' features — those features, which depend only on the document, but not on the query. For example, [[PageRank]] or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's ''static quality score'' (or ''static rank''), which is often used to speed up search query evaluation.&lt;ref name="manning-q-eval" /&gt;&lt;ref&gt;
{{cite conference
 | first=M. |last=Richardson |author2=Prakash, A. |author3=Brill, E.
 | title=Beyond PageRank: Machine Learning for Static Ranking
 | booktitle=Proceedings of the 15th International World Wide Web Conference
 | pages=707–715
 | publisher=
 | year=2006
 | url=http://research.microsoft.com/en-us/um/people/mattri/papers/www2006/staticrank.pdf
 | accessdate=
 }}&lt;/ref&gt;
* ''Query-dependent'' or ''dynamic'' features — those features, which depend both on the contents of the document and the query, such as [[TF-IDF]] score or other non-machine-learned ranking functions.
* ''Query level features'' or ''query features'', which depend only on the query. For example, the number of words in a query. ''Further information: [[query level feature]]''

Some examples of features, which were used in the well-known [[LETOR]] dataset:&lt;ref name="letor3"&gt;[http://research.microsoft.com/en-us/people/taoqin/letor3.pdf LETOR 3.0. A Benchmark Collection for Learning to Rank for Information Retrieval]&lt;/ref&gt;
* TF, [[TF-IDF]], [[Okapi BM25|BM25]], and [[language modeling]] scores of document's [[Zone (information retrieval)|zone]]s (title, body, anchors text, URL) for a given query;
* Lengths and [[Inverse document frequency|IDF]] sums of document's zones;
* Document's [[PageRank]], [[HITS algorithm|HITS]] ranks and their variants.

Selecting and designing good features is an important area in machine learning, which is called [[feature engineering]].

== Evaluation measures ==
{{main|Evaluation_measures_(information_retrieval)#Offline_metrics}}

There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare the performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.

Examples of ranking quality measures:
*[[Mean Average Precision|Mean average precision]] (MAP);
* [[Discounted cumulative gain|DCG]] and [[Normalized discounted cumulative gain|NDCG]];
* [[Precision (information retrieval)|Precision]]@''n'', NDCG@''n'', where "@''n''" denotes that the metrics are evaluated only on top ''n'' documents;
* [[Mean reciprocal rank]];
* [[Kendall's tau]];
* [[Spearman's rank correlation coefficient|Spearman's rho]].

DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.&lt;ref&gt;http://www.stanford.edu/class/cs276/handouts/lecture15-learning-ranking.ppt&lt;/ref&gt; Other metrics such as MAP, MRR and precision, are defined only for binary judgments.

Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:
* Expected reciprocal rank (ERR);&lt;ref&gt;{{citation
|author1=Olivier Chapelle |author2=Donald Metzler |author3=Ya Zhang |author4=Pierre Grinspan |title=Expected Reciprocal Rank for Graded Relevance
|url=http://research.yahoo.com/files/err.pdf |archive-url=https://web.archive.org/web/20120224053008/http://research.yahoo.com/files/err.pdf |url-status=dead |archive-date=2012-02-24 |journal=CIKM
|year=2009
}}&lt;/ref&gt;
* [[Yandex]]'s pfound.&lt;ref&gt;{{citation
|author1=Gulin A. |author2=Karpovich P. |author3=Raskovalov D. |author4=Segalovich I. |title=Yandex at ROMIP'2009: optimization of ranking algorithms by machine learning methods
|url=http://romip.ru/romip2009/15_yandex.pdf
|journal=Proceedings of ROMIP'2009
|year=2009
|pages=163–168
}} (in Russian)&lt;/ref&gt;
Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.

== Approaches ==
{{Expand section|date=December 2009}}
Tie-Yan Liu of [[Microsoft Research Asia]] has analyzed existing algorithms for learning to rank problems in his paper "Learning to Rank for Information Retrieval".&lt;ref name="liu" /&gt; He categorized them into three groups by their input representation and [[loss function]]: the pointwise, pairwise, and listwise approach. In practice, listwise approaches often outperform pairwise approaches and pointwise approaches. This statement was further supported by a large scale experiment on the performance of different learning-to-rank methods on a large collection of benchmark data sets.&lt;ref name="Tax2015"&gt;{{citation |author1=Tax, Niek |author2=Bockting, Sander |author3=Hiemstra, Djoerd |journal=Information Processing &amp; Management |volume=51 |issue=6 |title=A cross-benchmark comparison of 87 learning to rank methods |pages=757–772 |year=2015 |url=http://wwwhome.cs.utwente.nl/~hiemstra/papers/ipm2015.pdf |doi=10.1016/j.ipm.2015.07.002 |access-date=2017-10-15 |archive-url=https://web.archive.org/web/20170809115827/http://wwwhome.cs.utwente.nl/~hiemstra/papers/ipm2015.pdf |archive-date=2017-08-09 |url-status=dead }}&lt;/ref&gt;

=== Pointwise approach ===
In this case, it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then the learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score.

A number of existing [[Supervised learning|supervised]] machine learning algorithms can be readily used for this purpose. [[Ordinal regression]] and [[classification (machine learning)|classification]] algorithms can also be used in pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.

=== Pairwise approach ===
In this case, the learning-to-rank problem is approximated by a classification problem — learning a [[binary classifier]] that can tell which document is better in a given pair of documents. The goal is to minimize the average number of [[Permutation#Inversions|inversions]] in ranking.

=== Listwise approach ===
These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.

=== List of methods ===
A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:
:{|class="wikitable sortable"
! Year || Name || Type || Notes
|-
| 1989 || OPRF &lt;ref name="Fuhr1989"&gt;{{citation
 | last=Fuhr
 | first=Norbert
 | journal=ACM Transactions on Information Systems
 | title=Optimum polynomial retrieval functions based on the probability ranking principle
 | volume=7
 | number=3
 | pages=183–204 
 | year=1989
 | doi=10.1145/65943.65944
}}&lt;/ref&gt; || &lt;span style="display:none"&gt;2&lt;/span&gt; pointwise || Polynomial regression (instead of machine learning, this work refers to pattern recognition, but the idea is the same)
|-
| 1992 || SLR &lt;ref name="Cooperetal1992"&gt;{{citation
 |author1=Cooper, William S. |author2=Gey, Frederic C. |author3=Dabney, Daniel P. | journal=SIGIR '92 Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval 
 | title=Probabilistic retrieval based on staged logistic regression
 | pages=198–210 
 | year=1992
 | doi=10.1145/133160.133199
|isbn=978-0897915236 }}&lt;/ref&gt;   || &lt;span style="display:none"&gt;2&lt;/span&gt; pointwise || Staged logistic regression
|-
| 1999 || [http://www-stat.stanford.edu/~jhf/ftp/trebst.ps MART] (Multiple Additive Regression Trees) || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || 
|-
| 2000 || [http://research.microsoft.com/apps/pubs/default.aspx?id=65610 Ranking SVM] (RankSVM) || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||  A more recent exposition is in,&lt;ref name="Joachims2002" /&gt; which describes an application to ranking using clickthrough logs.
|-
| 2002 || Pranking&lt;ref&gt;{{cite journal | citeseerx = 10.1.1.20.378 | title = Pranking }}&lt;/ref&gt; || &lt;span style="display:none"&gt;1&lt;/span&gt; pointwise || Ordinal regression.
|-
| 2003 &lt;!-- or 1998? --&gt; || [http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf RankBoost] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||
|-
| 2005 || [https://www.microsoft.com/en-us/research/wp-content/uploads/2005/08/icml_ranking.pdf RankNet] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||
|-
| 2006 || [http://research.microsoft.com/en-us/people/tyliu/cao-et-al-sigir2006.pdf IR-SVM] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || Ranking SVM with query-level normalization in the loss function.
|-
| 2006 || [http://research.microsoft.com/en-us/um/people/cburges/papers/lambdarank.pdf LambdaRank] || pairwise/listwise || RankNet in which pairwise loss function is multiplied by the change in the IR metric caused by a swap.
|-
| 2007 || [http://research.microsoft.com/en-us/people/junxu/sigir2007-adarank.pdf AdaRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70364 FRank] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || Based on RankNet, uses a different loss function - fidelity loss.
|-
| 2007 || [http://www.cc.gatech.edu/~zha/papers/fp086-zheng.pdf GBRank] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || 
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70428 ListNet] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=68128 McRank] || &lt;span style="display:none"&gt;1&lt;/span&gt; pointwise ||
|-
| 2007 || [https://web.archive.org/web/20100807162456/http://www.stat.rutgers.edu/~tzhang/papers/nips07-ranking.pdf QBRank] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||
|-
| 2007 || [http://research.microsoft.com/en-us/people/hangli/qin_ipm_2008.pdf RankCosine] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || RankGP&lt;ref&gt;{{cite journal | citeseerx = 10.1.1.90.220 | title = RankGP }}&lt;/ref&gt; || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2007 || [http://staff.cs.utu.fi/~aatapa/publications/inpPaTsAiBoSa07a.pdf RankRLS] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise ||
Regularized least-squares based ranking. The work is extended in
&lt;ref name=pahikkala2009efficient&gt;{{Citation|last=Pahikkala|first=Tapio |author2=Tsivtsivadze, Evgeni |author3=Airola, Antti |author4=Järvinen, Jouni |author5=Boberg, Jorma |title=An efficient algorithm for learning to rank from preference graphs|journal=Machine Learning|year=2009|volume=75|issue=1|pages=129–165|doi=10.1007/s10994-008-5097-z|postscript=.|doi-access=free}}&lt;/ref&gt; to learning to rank from general preference graphs.
|-
| 2007 || [http://www.cs.cornell.edu/People/tj/publications/yue_etal_07a.pdf SVM&lt;sup&gt;map&lt;/sup&gt;] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://research.microsoft.com/pubs/69536/tr-2008-109.pdf LambdaSMART/LambdaMART]  || pairwise/listwise || Winning entry in the recent Yahoo Learning to Rank competition used an ensemble of LambdaMART models. Based on MART (1999)&lt;ref&gt;C. Burges. (2010). [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf From RankNet to LambdaRank to LambdaMART: An Overview].&lt;/ref&gt; “LambdaSMART”, for Lambda-submodel-MART, or LambdaMART for the case with no submodel [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2008-109.pdf (https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2008-109.pdf]).
|-
| 2008 || [http://research.microsoft.com/en-us/people/tyliu/icml-listmle.pdf ListMLE] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise || Based on ListNet.
|-
| 2008 || [http://research.microsoft.com/en-us/people/junxu/sigir2008-directoptimize.pdf PermuRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://research.microsoft.com/apps/pubs/?id=63585 SoftRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise ||
|-
| 2008 || [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf Ranking Refinement]&lt;ref&gt;Rong Jin, Hamed Valizadegan, Hang Li, [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf ''Ranking Refinement and Its Application for Information Retrieval''], in International Conference on World Wide Web (WWW), 2008.&lt;/ref&gt; || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || A semi-supervised approach to learning to rank that uses Boosting.
|-
| 2008 || [https://web.archive.org/web/20100723152841/http://www-connex.lip6.fr/~amini/SSRankBoost/ SSRankBoost]&lt;ref&gt;Massih-Reza Amini, Vinh Truong, Cyril Goutte, [http://www-connex.lip6.fr/~amini/Publis/SemiSupRanking_sigir08.pdf ''A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data''] {{Webarchive|url=https://web.archive.org/web/20100802093049/http://www-connex.lip6.fr/~amini/Publis/SemiSupRanking_sigir08.pdf |date=2010-08-02 }}, International ACM SIGIR conference, 2008. The [http://www-connex.lip6.fr/~amini/SSRankBoost/ code] {{Webarchive|url=https://web.archive.org/web/20100723152841/http://www-connex.lip6.fr/~amini/SSRankBoost/ |date=2010-07-23 }} is available for research purposes.&lt;/ref&gt;  || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise|| An extension of RankBoost to learn with partially labeled data (semi-supervised learning to rank)
|-
| 2008 || [http://phd.dii.unisi.it/PosterDay/2009/Tiziano_Papini.pdf SortNet]&lt;ref&gt;Leonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli, [http://research.microsoft.com/en-us/um/beijing/events/lr4ir-2008/PROCEEDINGS-LR4IR%202008.PDF "SortNet: learning to rank by a neural-based sorting algorithm"], SIGIR 2008 workshop: Learning to Rank for Information Retrieval, 2008&lt;/ref&gt; || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise|| SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. 
|-
| 2009 || [https://web.archive.org/web/20101122085504/http://itcs.tsinghua.edu.cn/papers/2009/2009031.pdf MPBoost] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || Magnitude-preserving variant of RankBoost. The idea is that the more unequal are labels of a pair of documents, the harder should the algorithm try to rank them.
|-
| 2009 || [https://web.archive.org/web/20130620070239/http://machinelearning.org/archive/icml2009/papers/498.pdf BoltzRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise || Unlike earlier methods, BoltzRank produces a ranking model that looks during query time not just at a single document, but also at pairs of documents.
|-
| 2009 || [http://www.iis.sinica.edu.tw/papers/whm/8820-F.pdf BayesRank] || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise || A method combines Plackett-Luce Model and neural network to minimize the expected Bayes risk, related to NDCG, from the decision-making aspect.
|-
| 2010 || [https://people.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf NDCG Boost]&lt;ref&gt;Hamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao, [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf ''Learning to Rank by Optimizing NDCG Measure''], in Proceeding of Neural Information Processing Systems (NIPS), 2010.&lt;/ref&gt; || &lt;span style="display:none"&gt;3&lt;/span&gt; listwise || A boosting approach to optimize NDCG.
|-
| 2010 || [https://arxiv.org/abs/1001.4597 GBlend] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise || Extends GBRank to the learning-to-blend problem of jointly solving multiple learning-to-rank problems with some shared features.
|-
| 2010 || [https://web.archive.org/web/20100601205607/http://wume.cse.lehigh.edu/~ovd209/wsdm/proceedings/docs/p151.pdf IntervalRank] || &lt;span style="display:none"&gt;2&lt;/span&gt; pairwise &amp; listwise || 
|-
| 2010 || [http://www.eecs.tufts.edu/~dsculley/papers/combined-ranking-and-regression.pdf CRR] || &lt;span style="display:none"&gt;2&lt;/span&gt; pointwise &amp; pairwise || Combined Regression and Ranking. Uses [[stochastic gradient descent]] to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from Ranking SVM.
|-
|2016
|[https://arxiv.org/abs/1603.02754 XGBoost]
|pairwise
|Supports various ranking objectives and evaluation metrics.
|-
|2017 || [http://eprints.nottingham.ac.uk/41540/1/dls_sac2017.pdf ES-Rank] || listwise || Evolutionary Strategy Learning to Rank technique with 7 fitness evaluation metrics
|-
|2018
|[http://www.jmlr.org/papers/volume19/17-179/17-179.pdf PolyRank]&lt;ref&gt;{{Cite journal|last=Davidov|first=Ori|last2=Ailon|first2=Nir|last3=Oliveira|first3=Ivo F. D.|date=2018|title=A New and Flexible Approach to the Analysis of Paired Comparison Data|url=http://jmlr.org/papers/v19/17-179.html|journal=Journal of Machine Learning Research|volume=19|issue=60|pages=1–29|issn=1533-7928}}&lt;/ref&gt;
|pairwise
|Learns simultaneously the ranking and the underlying generative model from pairwise comparisons.
|-
|2018 || [https://arxiv.org/abs/1803.05796 FATE-Net/FETA-Net] &lt;ref&gt;{{cite arXiv |last=Pfannschmidt |first=Karlson |last2=Gupta |first2=Pritha | last3=Hüllermeier |first3=Eyke |date=2018 |title=Deep Architectures for Learning Context-dependent Ranking Functions |class=stat.ML |eprint=1803.05796}}&lt;/ref&gt;|| listwise || End-to-end trainable architectures, which explicitly take all items into account to model context effects.
|-
|2019
|[http://cs-people.bu.edu/fcakir/papers/fastap_cvpr2019.pdf FastAP] &lt;ref&gt;Fatih Cakir, Kun He, Xide Xia, Brian Kulis, Stan Sclaroff, [http://cs-people.bu.edu/fcakir/papers/fastap_cvpr2019.pdf ''Deep Metric Learning to Rank''], In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.&lt;/ref&gt;
|listwise
|Optimizes Average Precision to learn deep embeddings
|-
|2019
|[https://arxiv.org/abs/1905.06452 Mulberry] || listwise &amp; hybrid || Learns ranking policies maximizing multiple metrics across the entire dataset
|}

Note: as most [[supervised learning]] algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.

== History ==
[[Norbert Fuhr]] introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;&lt;ref name="Fuhr1992"&gt;{{citation
 | last=Fuhr
 | first=Norbert
 | journal=Computer Journal
 | title=Probabilistic Models in Information Retrieval
 | volume=35
 | number=3
 | pages=243–255
 | year=1992
 | doi=10.1093/comjnl/35.3.243
| doi-access=free
 }}&lt;/ref&gt; a specific variant of this approach (using [[polynomial regression]]) had been published by him three years earlier.&lt;ref name="Fuhr1989" /&gt; Bill Cooper proposed [[logistic regression]] for the same purpose in 1992 &lt;ref name="Cooperetal1992" /&gt; and used it with his  [[University of California at Berkeley|Berkeley]] research group to train a successful ranking function for [[Text Retrieval Conference|TREC]].  Manning et al.&lt;ref&gt;{{citation |author1=Manning C. |author2=Raghavan P. |author3=Schütze H. |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008}}. Sections [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-7.html 7.4] and [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html 15.5]&lt;/ref&gt;  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.

Several conferences, such as [[Neural Information Processing Systems|NIPS]], [[Special Interest Group on Information Retrieval|SIGIR]] and [[International Conference on Machine Learning|ICML]] had workshops devoted to the learning-to-rank problem since mid-2000s (decade).

=== Practical usage by search engines ===
Commercial [[web search engine]]s began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was [[AltaVista]] (later its technology was acquired by [[Overture Services, Inc.|Overture]], and then [[Yahoo]]), which launched a [[gradient boosting]]-trained ranking function in April 2003.&lt;ref&gt;Jan O. Pedersen. [http://jopedersen.com/Presentations/The_MLR_Story.pdf The MLR Story] {{Webarchive|url=https://web.archive.org/web/20110713120113/http://jopedersen.com/Presentations/The_MLR_Story.pdf |date=2011-07-13 }}&lt;/ref&gt;&lt;ref&gt;{{US Patent|7197497}}&lt;/ref&gt;

[[Bing (search engine)|Bing]]'s search is said to be powered by [https://www.microsoft.com/en-us/research/wp-content/uploads/2005/08/icml_ranking.pdf RankNet] algorithm,&lt;ref&gt;[http://www.bing.com/community/blogs/search/archive/2009/06/01/user-needs-features-and-the-science-behind-bing.aspx?PageIndex=4 Bing Search Blog: User Needs, Features and the Science behind Bing]&lt;/ref&gt;{{when|date=February 2014}} which was invented at [[Microsoft Research]] in 2005.

In November 2009 a Russian search engine [[Yandex]] announced&lt;ref name="snezhinsk"&gt;[http://webmaster.ya.ru/replies.xml?item_no=5707&amp;ncrnd=5118 Yandex corporate blog entry about new ranking model "Snezhinsk"] (in Russian)&lt;/ref&gt; that it had significantly increased its [[search quality]] due to deployment of a new proprietary [[MatrixNet]] algorithm, a variant of [[gradient boosting]] method which uses [[oblivious decision tree]]s.&lt;ref&gt;The algorithm wasn't disclosed, but a few details were made public in [http://download.yandex.ru/company/experience/GDD/Zadnie_algoritmy_Karpovich.pdf] and [http://download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf].&lt;/ref&gt; Recently they have also sponsored a machine-learned ranking competition "Internet Mathematics 2009"&lt;ref&gt;{{Cite web |url=http://imat2009.yandex.ru/academic/mathematic/2009/en/ |title=Yandex's Internet Mathematics 2009 competition page |access-date=2009-11-11 |archive-url=https://web.archive.org/web/20150317144535/http://imat2009.yandex.ru/academic/mathematic/2009/en/ |archive-date=2015-03-17 |url-status=dead }}&lt;/ref&gt; based on their own search engine's production data. Yahoo has announced a similar competition in 2010.&lt;ref&gt;{{Cite web |url=http://learningtorankchallenge.yahoo.com/ |title=Yahoo Learning to Rank Challenge |access-date=2010-02-26 |archive-url=https://web.archive.org/web/20100301011649/http://learningtorankchallenge.yahoo.com/ |archive-date=2010-03-01 |url-status=dead }}&lt;/ref&gt;

As of 2008, [[Google]]'s [[Peter Norvig]] denied that their search engine exclusively relies on machine-learned ranking.&lt;ref&gt;{{cite web
 |url         = http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-prone-to-catastrophic-errors-than-machine-learned-models.html
 |archiveurl  = https://www.webcitation.org/5sq8irWNM?url=http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-prone-to-catastrophic-errors-than-machine-learned-models.html
 |archivedate = 2010-09-18
 |title       = Are Machine-Learned Models Prone to Catastrophic Errors?
 |date        = 2008-05-24
 |last        = Rajaraman
 |first       = Anand
 |authorlink  = Anand Rajaraman
 |access-date = 2009-11-11
 |url-status    = live
}}&lt;/ref&gt; [[Cuil]]'s CEO, [[Tom Costello (businessman)|Tom Costello]], suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models "learn what people say they like, not what people actually like".&lt;ref&gt;{{cite web
  | url = http://www.cuil.com/info/blog/2009/06/26/so-how-is-bing-doing
  | archiveurl = https://archive.is/20090627213358/http://www.cuil.com/info/blog/2009/06/26/so-how-is-bing-doing
  | archivedate = 2009-06-27
  | title = Cuil Blog: So how is Bing doing?
  | date = 2009-06-26
  | last = Costello
  | first = Tom}}&lt;/ref&gt;

In January 2017 the technology was included in the [[Open-source software|open source]] search engine [[Apache Solr]]™,&lt;ref&gt;{{Cite news|url=https://www.techatbloomberg.com/blog/bloomberg-integrated-learning-rank-apache-solr/|title=How Bloomberg Integrated Learning-to-Rank into Apache Solr {{!}} Tech at Bloomberg|date=2017-01-23|work=Tech at Bloomberg|access-date=2017-02-28|language=en-US}}&lt;/ref&gt; thus making machine learned search rank widely accessible also for enterprise search.

== References ==
{{reflist|2}}

== External links ==
; Competitions and public datasets
* [https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/ LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval]
* [https://web.archive.org/web/20150912134134/http://imat2009.yandex.ru/en Yandex's Internet Mathematics 2009]
* [https://web.archive.org/web/20100301011649/http://learningtorankchallenge.yahoo.com/ Yahoo! Learning to Rank Challenge]
* [http://research.microsoft.com/en-us/projects/mslr/default.aspx Microsoft Learning to Rank Datasets]

; Open Source code
* [https://mloss.org/software/view/332/ Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011]
* [https://sites.google.com/site/rtranking/ C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking]
* [http://dlib.net/ml.html#svm_rank_trainer C++ and Python tools for using the SVM-Rank algorithm]
* [https://github.com/apache/lucene-solr/tree/master/solr/contrib/ltr Java implementation in the Apache Solr search engine]

[[Category:Information retrieval techniques]]
[[Category:Machine learning]]
[[Category:Ranking functions]]</text>
      <sha1>14qm11en2pqk5fqhz0mtkjdp1p5mky3</sha1>
    </revision>
  </page>
</mediawiki>
