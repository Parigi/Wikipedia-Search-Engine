<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.35.0-wmf.36</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Numerical methods for linear least squares</title>
    <ns>0</ns>
    <id>30035928</id>
    <revision>
      <id>850482936</id>
      <parentid>850480044</parentid>
      <timestamp>2018-07-16T04:23:26Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>removed [[Category:Numerical analysis]]; added [[Category:Numerical linear algebra]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text bytes="10244" xml:space="preserve">'''Numerical methods for linear least squares''' entails the [[numerical analysis]] of [[linear least squares]] problems.

==Introduction==
A general approach to the least squares problem &lt;math&gt;\operatorname{\,min} \, \big\|\mathbf y - X \boldsymbol \beta \big\|^2&lt;/math&gt; can be described as follows. Suppose that we can find an ''n''  by ''m''   matrix '''S''' 
such that '''XS''' is an      
[[Linear projection|orthogonal projection]] onto the image of '''X'''. Then a solution to our  minimization problem is  given by

:&lt;math&gt;\boldsymbol \beta  =  S   \mathbf y  &lt;/math&gt;

simply because

:&lt;math&gt;  X \boldsymbol \beta  =  X ( S   \mathbf y) = (X  S)  \mathbf y&lt;/math&gt;

is exactly a sought for orthogonal projection of &lt;math&gt;  \mathbf y &lt;/math&gt; onto an image of '''X''' 
([[#Properties_of_the_least-squares_estimators|see the picture below]] and  note that as  explained in the
[[#Properties_of_the_least-squares_estimators|next section]] the image of '''X''' is just a subspace generated by column vectors of  '''X'''). 
A few popular ways to find such a matrix ''S'' are described below.

==Inverting the matrix of the normal equations==
The algebraic solution of the normal equations with a full-rank matrix ''X''&lt;sup&gt;T&lt;/sup&gt;''X'' can be written as

: &lt;math&gt; \hat{\boldsymbol{\beta}} = (\mathbf X^ {\rm T} \mathbf X )^{-1} \mathbf X^ {\rm T} \mathbf y 
= \mathbf X^+ \mathbf y&lt;/math&gt;

where ''X''&lt;sup&gt;+&lt;/sup&gt; is the [[Moore–Penrose pseudoinverse]] of ''X''. Although this equation is correct and can work in many applications, it is not computationally efficient to invert the normal-equations matrix (the [[Gramian matrix]]). An exception occurs in [[numerical smoothing and differentiation]] where an analytical expression is required.

If the matrix ''X''&lt;sup&gt;T&lt;/sup&gt;''X'' is [[Condition number|well-conditioned]] and [[Positive-definite matrix|positive definite]], implying that it has full [[rank (linear algebra)|rank]], the normal equations can be solved directly by using the [[Cholesky decomposition]] ''R''&lt;sup&gt;T&lt;/sup&gt;''R'', where ''R'' is an upper [[triangular matrix]], giving:

: &lt;math&gt; R^{\rm T} R \hat{\boldsymbol{\beta}} =  X^{\rm T} \mathbf y. &lt;/math&gt;

The solution is obtained in two stages, a [[forward substitution]] step, solving for '''z''':

: &lt;math&gt; R^{\rm T} \mathbf z = X^{\rm T} \mathbf y,&lt;/math&gt;

followed by a backward substitution, solving for &lt;math&gt;\hat{\boldsymbol{\beta}}&lt;/math&gt;:

: &lt;math&gt;R \hat{\boldsymbol{\beta}}= \mathbf z.&lt;/math&gt;

Both substitutions are facilitated by the triangular nature of ''R''.

==Orthogonal decomposition methods==
Orthogonal decomposition methods of solving the least squares problem are slower than the normal equations method but are more [[Numerical stability|numerically stable]] because they avoid forming the product ''X''&lt;sup&gt;T&lt;/sup&gt;''X''.

The residuals are written in matrix notation as

:&lt;math&gt;\mathbf r= \mathbf y - X \hat{\boldsymbol{\beta}}.&lt;/math&gt;

The matrix ''X'' is subjected to an orthogonal decomposition, e.g., the [[QR decomposition]] as follows. 
:&lt;math&gt;X=Q
\begin{pmatrix}
R \\
0
\end{pmatrix} 
\ &lt;/math&gt;,
where ''Q'' is an ''m''×''m'' [[orthogonal matrix]] (''Q''&lt;sup&gt;T&lt;/sup&gt;''Q=I'') and ''R'' is an ''n''×''n'' upper triangular matrix with &lt;math&gt;r_{ii}&gt;0&lt;/math&gt;.

The residual vector is left-multiplied by ''Q''&lt;sup&gt;T&lt;/sup&gt;.

:&lt;math&gt;Q^{\rm T} \mathbf r = Q^{\rm T} \mathbf y - \left( Q^{\rm T} Q \right) 
\begin{pmatrix}
R \\
0
\end{pmatrix} 
\hat{\boldsymbol{\beta}}= \begin{bmatrix}
\left(Q^{\rm T} \mathbf y \right)_n - R \hat{\boldsymbol{\beta}}  \\
\left(Q^{\rm T} \mathbf y \right)_{m-n} 
\end{bmatrix}
= \begin{bmatrix}
\mathbf u \\
\mathbf v
\end{bmatrix}
&lt;/math&gt;

Because ''Q'' is [[orthogonal matrix|orthogonal]], the sum of squares of the residuals, ''s'', may be written as:
:&lt;math&gt;s = \|\mathbf r \|^2 = \mathbf r^{\rm T} \mathbf r = \mathbf r^{\rm T} Q Q^{\rm T} \mathbf r = \mathbf u^{\rm T} \mathbf u + \mathbf v^{\rm T} \mathbf v &lt;/math&gt;
Since '''v''' doesn't depend on '''''β''''', the minimum value of ''s'' is attained when the upper block, '''u''', is zero. Therefore, the parameters are found by solving:
:&lt;math&gt; R \hat{\boldsymbol{\beta}} =\left(Q^{\rm T} \mathbf y \right)_n.&lt;/math&gt;
These equations are easily solved as ''R'' is upper triangular.

An alternative decomposition of ''X'' is the [[singular value decomposition]] (SVD)&lt;ref&gt;{{cite book |title=Solving Least Squares Problems |last=Lawson |first=C. L. |authorlink= |author2=Hanson, R. J.  |year=1974 |publisher=Prentice-Hall |location=Englewood Cliffs, NJ |isbn=0-13-822585-0 |pages= |url= }}&lt;/ref&gt;

:&lt;math&gt; X = U \Sigma V^{\rm T} \ &lt;/math&gt;,

where ''U'' is ''m'' by ''m'' orthogonal matrix,  ''V'' is ''n'' by ''n'' orthogonal matrix and &lt;math&gt;\Sigma&lt;/math&gt; is an ''m'' by ''n'' matrix with all its elements outside of the main diagonal   equal to ''0''. The [[pseudoinverse]] of &lt;math&gt;\Sigma&lt;/math&gt; is easily obtained by inverting its non-zero diagonal elements and transposing. Hence,

:&lt;math&gt;  \mathbf X \mathbf X^+ = U \Sigma V^{\rm T}  V \Sigma^+  U^{\rm T} =  U P  U^{\rm T},&lt;/math&gt;

where ''P'' is obtained from &lt;math&gt;\Sigma&lt;/math&gt; by replacing its non-zero diagonal elements with ones. Since &lt;math&gt;(\mathbf X \mathbf X^+)^* = \mathbf X \mathbf X^+ &lt;/math&gt; (the property of pseudoinverse), the matrix &lt;math&gt;U P U^{\rm T}&lt;/math&gt; is an orthogonal projection onto the image (column-space) of ''X''. In accordance with a general approach described in the introduction above (find '''XS''' which is an orthogonal projection),

:&lt;math&gt; S = \mathbf X^+ &lt;/math&gt;,

and thus,

:&lt;math&gt; \beta = V\Sigma^+ U^{\rm T} \mathbf y &lt;/math&gt;

is a solution of a least squares problem. This method is the most computationally intensive, but is particularly useful if the normal equations matrix, ''X''&lt;sup&gt;T&lt;/sup&gt;''X'', is very ill-conditioned (i.e. if its [[condition number]] multiplied by the machine's relative [[round-off error]] is appreciably large).  In that case, including the smallest [[singular value]]s in the inversion merely adds numerical noise to the solution.  This can be cured with the truncated SVD approach, giving a more stable and exact answer, by explicitly setting to zero all singular values below a certain threshold and so ignoring them, a process closely related to [[factor analysis]].

==Discussion==
The numerical methods for linear least squares are important because [[linear regression]] models are among the most important types of model, both as formal [[statistical model]]s and for exploration of data-sets. The majority of [[Comparison of statistical packages|statistical computer packages]] contain facilities for regression analysis that make use of linear least squares computations. Hence it is appropriate that considerable effort has been devoted to the task of ensuring that these computations are undertaken efficiently and with due regard to [[round-off error]].

Individual statistical analyses are seldom undertaken in isolation, but rather are part of a sequence of investigatory steps. Some of the topics involved in considering numerical methods for linear least squares relate to this point. Thus important topics can be
*Computations where a number of similar, and often [[Statistical model#Nested models|nested]], models are considered for the same data-set. That is, where models with the same [[dependent variable]] but different sets of [[independent variables]] are to be considered, for essentially the same set of data-points.
*Computations for analyses that occur in a sequence, as the number of data-points increases.
*Special considerations for very extensive data-sets.

Fitting of linear models by least squares often, but not always, arise in the context of [[statistical analysis]]. It can therefore be important that considerations of computation efficiency for such problems extend to all of the auxiliary quantities required for such analyses, and are not restricted to the formal solution of the linear least squares problem.

Matrix calculations, like any other, are affected by [[rounding error]]s. An early summary of these effects, regarding the choice of computation methods for matrix inversion, was provided by Wilkinson.&lt;ref&gt;Wilkinson, J.H. (1963) "Chapter 3: Matrix Computations", ''Rounding Errors in Algebraic Processes'', London: Her Majesty's Stationery  Office (National Physical Laboratory, Notes in Applied Science, No.32)&lt;/ref&gt;

==See also==
*[[Numerical linear algebra]]
*[[Numerical methods for non-linear least squares]]

==References==
{{reflist}}

==Further reading==
*Ake Bjorck, ''Numerical Methods for Least Squares Problems'', SIAM, 1996.
*R. W. Farebrother, ''Linear Least Squares Computations'', CRC Press, 1988.
*{{Citation
 | last=Barlow
 | first=Jesse L.
 | author-link=
 | chapter=Chapter 9: Numerical aspects of Solving Linear Least Squares Problems
 | editor-last=Rao  | editor-first=C. R.
 | title=Computational Statistics  |  series=Handbook of Statistics  | volume=9
 | publisher=North-Holland
 | publication-date=1993
 | isbn=0-444-88096-8
 }}
*{{Cite book | last1=Björck |first1= Åke | authorlink= | title=Numerical methods for least squares problems | year=1996 | publisher=SIAM | location=Philadelphia  | isbn=0-89871-360-9 | pages=}}
*{{Citation
 | last=Goodall
 | first=Colin R.
 | author-link=
 | chapter=Chapter 13: Computation using the QR decomposition
 | editor-last=Rao  | editor-first=C. R.
 | title=Computational Statistics  |  series=Handbook of Statistics  | volume=9
 | publisher=North-Holland
 | publication-date=1993
 | isbn=0-444-88096-8
 }}
*{{Citation
 | last=National Physical Laboratory
 | first=
 | chapter=Chapter 1: Linear Equations and Matrices: Direct Methods
 | title=Modern Computing Methods
 | edition=2nd
 | series=Notes on Applied Science
 | volume=16
 | publisher=Her Majesty's Stationery Office
 | publication-date=1961
 }}
*{{Citation
 | last=National Physical Laboratory
 | first=
 | chapter=Chapter 2: Linear Equations and Matrices: Direct Methods on Automatic Computers
 | title=Modern Computing Methods
 | edition=2nd
 | series=Notes on Applied Science
 | volume=16
 | publisher=Her Majesty's Stationery Office
 | publication-date=1961
 }}


[[Category:Numerical linear algebra|Least squares]]
[[Category:Least squares]]</text>
      <sha1>6tqyrb5ljk1u05o9wshjo9j5s2gkeb2</sha1>
    </revision>
  </page>
</mediawiki>
